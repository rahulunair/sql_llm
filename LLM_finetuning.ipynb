{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc557a5-5a55-4bae-8555-a2b655abfa4a",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c23f99-bfcf-476b-91d5-4f2ec1178707",
   "metadata": {},
   "source": [
    "# Text to SQL Generation: Fine-Tuning LLMs with QLoRA on Intel\n",
    "\n",
    "Hello and welcome! In this Jupyter Notebook, we will walkthrough the process of fine-tuning a large language model (LLM) to improve its capabilities in generating SQL queries from natural language input. The notebook is suitable for AI engineers and practitioners looking to tune LLMs for specialized tasks such as Text-to-SQL conversions.\n",
    "\n",
    "### What you will learn with this Notebook\n",
    "\n",
    "- Fine-tune a Language Model with either a pre-existing dataset or a custom dataset tailored to your needs on Intel Hw.\n",
    "- Gain insights into the fine-tuning process, including how to manipulate various training parameters to optimize your model's performance.\n",
    "- Test different configurations and observe the results in real-time.\n",
    "\n",
    "### Hardware Compatibility\n",
    "\n",
    "- The notebook utilizes both 4th Generation Intel® Xeon® Scalable Processors (cpu) and Intel® Data Center GPU Max Series 1100 (xpu).\n",
    "\n",
    "### Fine-Tuning with QLoRA: Balancing Memory Efficiency and Adaptability\n",
    "\n",
    "We leverage the QLoRA methodology for fine-tuning, enabling the loading and refinement of LLMs within the constraints of available GPU memory. QLoRA, which stands for Quantized Low Rank Adapter, achieves this by applying a clever combination of weight quantization and adapter-based finetuning.\n",
    "\n",
    "The core idea behind QLoRA is to reduce the memory footprint by quantizing the weights of a pre-trained model—compressing them down to a fraction of their original size. This quantization allows us to operate with models that would otherwise exceed our hardware limits. During the fine-tuning process, we optimize only the adapter parameters, which are low-rank matrices specifically designed to update the model for our target task without the need to retrain the entire network. This selective training approach also contributes to computational efficiency, as it narrows down the number of trainable parameters.\n",
    "\n",
    "Through this technique, we can adapt a model to generate SQL queries from natural language with minimal overhead. The BigDL library, Hugging Face Transformers and the peft library provide the necessary tools for this process, facilitating the fine-tuning of language models in a way that is both resource-conscious and effective.\n",
    "\n",
    "Let's begin our journey into fine-tuning a model that can adeptly translate natural language queries into SQL statements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362a562-f3ce-4dce-9678-ce317e554a04",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Let's first install and import all the necessary libraries required for the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "import subprocess\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "def get_python_version():\n",
    "    return \"python\" + \".\".join(map(str, sys.version_info[:2]))\n",
    "\n",
    "\n",
    "def is_package_installed(pkg_name):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(pkg_name)\n",
    "        return True\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False\n",
    "\n",
    "\n",
    "def set_local_bin_path():\n",
    "    local_bin = str(Path.home() / \".local\" / \"bin\")\n",
    "    local_site_packages = str(\n",
    "        Path.home() / \".local\" / \"lib\" / get_python_version() / \"site-packages\"\n",
    "    )\n",
    "    sys.path.append(local_bin)\n",
    "    sys.path.insert(0, site.getusersitepackages())\n",
    "    sys.path.insert(0, sys.path.pop(sys.path.index(local_site_packages)))\n",
    "\n",
    "\n",
    "def install_pkgs(pkgs_dict=None):\n",
    "    set_local_bin_path()\n",
    "    uniq_dir = f\"pycache_{uuid.uuid4().hex}\"\n",
    "    cache_dir = str(Path(\"/tmp\") / uniq_dir)\n",
    "    if cache_dir:\n",
    "        Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if pkgs_dict is not None:\n",
    "        for pkg_name, pkg_url in pkgs_dict.items():\n",
    "            if is_package_installed(pkg_name):\n",
    "                print(f'Package \"{pkg_name}\" is already installed. Skipping.')\n",
    "                continue\n",
    "            cmd = [\n",
    "                sys.executable,\n",
    "                \"-m\",\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"--upgrade\",\n",
    "                \"--cache-dir\",\n",
    "                cache_dir,\n",
    "            ] + pkg_url.split()\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    print(f'Attempting to install: \"{pkg_name}\", please wait...')\n",
    "                    subprocess.run(\n",
    "                        cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "                    )\n",
    "                    print(f'\"{pkg_name}\" installed successfully.')\n",
    "                    break\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(\n",
    "                        f'Attempt {attempt + 1}: Error installing \"{pkg_name}\": {e.stderr.decode().strip()}'\n",
    "                    )\n",
    "                    if \"OSError\" in e.stderr.decode() and attempt < 2:\n",
    "                        print(\"Retrying due to OSError...\")\n",
    "                    else:\n",
    "                        print(f'Final attempt failed for \"{pkg_name}\".')\n",
    "                        break\n",
    "    if cache_dir:\n",
    "        subprocess.run([\"rm\", \"-rf\", cache_dir], check=False)\n",
    "\n",
    "\n",
    "pkgs_dict = {\n",
    "    \"bigdl-llm\": \"--pre --upgrade bigdl-llm[xpu]==2.4.0b20231116 -f https://developer.intel.com/ipex-whl-stable-xpu\",\n",
    "    \"peft\": \"peft==0.5.0\",\n",
    "    \"transformers\": \"transformers==4.34.0\",\n",
    "    \"accelerate\": \"accelerate==0.23.0\",\n",
    "    \"datasets\": \"datasets\",\n",
    "    \"wandb\": \"wandb\",\n",
    "}\n",
    "\n",
    "install_pkgs(pkgs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb9cf2-abcf-48f8-918b-37a18f85ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from math import ceil\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"intel_extension_for_pytorch\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"torchvision.io.image\", lineno=13\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are using the default legacy behaviour\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Parameter.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"This implementation of AdamW is deprecated\",\n",
    ")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"28\"\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bigdl\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from bigdl.llm.transformers.qlora import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training as prepare_model,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from bigdl.llm.transformers.qlora import PeftModel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b08285",
   "metadata": {},
   "source": [
    "## Note on Model Storage Management\n",
    "\n",
    "A set of LLM foundation models out-of-the-box as sated above. However, if you're interested in experimenting with additional models, consider the following guidelines:\n",
    "\n",
    "- **Storage Limitations:** Every user has a default free storage quota. You'll need to ensure that you have enough available space for the models you want to use, especially if your local directory is already storing data.\n",
    "- **Model Supported by PEFT:** If the model you're interested in is supported by the `peft` library by default, the necessary LoRA target modules are predefined and can be found in the [PEFT repository](https://github.com/huggingface/peft/blob/main/src/peft/utils/other.py#L434).\n",
    "- **Using Custom Models:** For models not natively supported by `peft`, you'll need to manually set the LoRA target modules in the `LoraConfig`. As an example, for llama models, the relevant target modules would be: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`.\n",
    "- **Checking and Retrieving Disk Space:** To check the available disk space where Hugging Face models are cached, use the Python function. If you need to free up space, you can delete the cache directory, but be aware that this will remove all downloaded models and they will have to be re-downloaded when needed next time.\n",
    "- Also rest the variable in the **Model Configuration** cell to `MODEL_CACHE_PATH = \"~/\"`\n",
    "\n",
    "### Python Function to Check Disk Space\n",
    "\n",
    "```python\n",
    "# Function to check available disk space in the Hugging Face cache directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_disk_space(path=\"~/.cache/huggingface/\"):\n",
    "    abs_path = os.path.expanduser(path)\n",
    "    total, used, free = shutil.disk_usage(abs_path)\n",
    "    print(f\"Total: {total // (2**30)} GiB\")\n",
    "    print(f\"Used: {used // (2**30)} GiB\")\n",
    "    print(f\"Free: {free // (2**30)} GiB\")\n",
    "\n",
    "# Example usage\n",
    "check_disk_space()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131864cb-ce5d-405b-8886-5d0f1f487c30",
   "metadata": {},
   "source": [
    "## Tailoring Your Model Configuration\n",
    "\n",
    "Welcome to the heart of customization in model fine-tuning. As you embark on this practical journey, you have a suite of base models at your disposal. Each has unique strengths, and the choice depends on your specific finetuning goals.\n",
    "\n",
    "- **Your Model Options**: Within the `BASE_MODELS`, you’ll find options ranging from the nimble\n",
    "  `open_llama_7b_v2` to the more expansive `Llama-2-13b-hf`, and specialized variants like `CodeLlama-7b-hf`.\n",
    "  Feel free to switch between these models to discover which one aligns best with your objectives.\n",
    "\n",
    "- **Dataset**: We will be using `b-mc2/sql-create-context` dataset from Huggingface datasets, that encompasses 78,577 examples, including natural language queries, SQL CREATE TABLE statements, and corresponding SQL queries. This dataset, extending from WikiSQL and Spider, is designed with text-to-SQL models in mind. It provides the structure needed for models to understand and generate accurate SQL statements. More details on the dataset can be found [here](https://huggingface.co/datasets/b-mc2/sql-create-context).\n",
    "- **LoRA Parameters - Your Knobs to Turn**:\n",
    "  - `r` (Rank): This is a key factor in how finely your model can adapt. A higher rank can grasp more\n",
    "    complex nuances, while a lower rank ensures a leaner memory footprint.\n",
    "  - `lora_alpha` (Scaling Factor): Think of this as the volume control for the LoRA adapters’ influence\n",
    "    on your model. Adjusting `lora_alpha` helps you fine-tune the balance between adaptation and preserving\n",
    "    the integrity of the pre-trained weights.\n",
    "  - `target_modules`: You decide which parts of the transformer model to enhance with LoRA adapters,\n",
    "    directly impacting how your model interprets and generates language.\n",
    "  - `lora_dropout`: A parameter that helps your model stay robust against overfitting. You can experiment\n",
    "    with different rates to achieve the best generalization.\n",
    "  - `bias`: Tweak the bias configurations to see their effect on your model’s learning dynamics.\n",
    "\n",
    "This notebook is set to start with `open_llama_7b_v2` as the default model, striking a balance suitable for a wide range of tasks. However, this journey is yours, and the real power lies with you to explore and fine-tune these settings to perfection on your chosen hardware. To use models like Llama 2, you will have to accept the usage policy as stipulated [here](https://ai.meta.com/llama/use-policy/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88077cc2-8fcf-4128-ac44-9fb2bb327398",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODELS = {\n",
    "    #\"0\": \"openlm-research/open_llama_3b_v2\",  # https://huggingface.co/openlm-research/open_llama_3b_v2\n",
    "    #\"1\": \"openlm-research/open_llama_13b\",  # https://huggingface.co/openlm-research/open_llama_13b\n",
    "    \"1\": \"openlm-research/open_llama_7b_v2\",  # https://huggingface.co/openlm-research/open_llama_7b_v2\n",
    "    \"2\": \"NousResearch/Nous-Hermes-Llama-2-7b\",  # https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b\n",
    "    \"3\": \"NousResearch/Llama-2-7b-chat-hf\",  # https://huggingface.co/NousResearch/Llama-2-7b-chat-hf\n",
    "    \"4\": \"NousResearch/Llama-2-13b-hf\",  # https://huggingface.co/NousResearch/Llama-2-13b-hf\n",
    "    \"5\": \"NousResearch/CodeLlama-7b-hf\",  # https://huggingface.co/NousResearch/CodeLlama-7b-hf\n",
    "    \"6\": \"Phind/Phind-CodeLlama-34B-v2\",  # https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\n",
    "}\n",
    "BASE_MODEL = BASE_MODELS[\"1\"]\n",
    "DATA_PATH = \"b-mc2/sql-create-context\"\n",
    "MODEL_PATH = \"./final_model\"\n",
    "ADAPTER_PATH = \"./lora_adapters\"\n",
    "DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "MODEL_CACHE_PATH = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Final model will be saved to: {MODEL_PATH}\")\n",
    "print(f\"LoRA adapters will be saved to: {ADAPTER_PATH}\")\n",
    "print(f\"Finetuning Model: {BASE_MODEL}\")\n",
    "print(f\"Using dataset from: {DATA_PATH}\")\n",
    "print(f\"Model cache: {MODEL_CACHE_PATH}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797b621-fd06-4ae9-a883-d7d15f16d6c4",
   "metadata": {},
   "source": [
    "## Prompt Engineering for Text-to-SQL Conversion\n",
    "\n",
    "In the realm of fine-tuning language models for specialized tasks, the design of the prompt is pivotal. The function `generate_prompt_sql` plays a crucial role in this process, particularly for the task of converting natural language questions into SQL queries.The `generate_prompt_sql` function is crafted to encapsulate the input question, the relevant database context, and the expected output in a structured and concise manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecd7df-b7ce-48b0-ba9f-04b7ec0c1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Configuration\n",
    "\n",
    "When initializing the `FineTuner` below, we load the base model specified by the `base_model_id`. Crucially, we use the `load_in_low_bit` parameter made possible by the BigDL library with the value \"nf4\", which allows the model to load in a 4-bit format, significantly reducing memory footprint while maintaining performance. The LoRA adapters is also set to use a `torch_dtype` of `torch.float16`, enabling mixed-precision training for faster computation and lower memory usage. The function `setup_model_and_tokenizer` is used to load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(base_model_id: str):\n",
    "    \"\"\"Downloads / Loads the pre-trained model in NF4 datatype and tokenizer based on the given base model ID for training.\"\"\"\n",
    "    local_model_id = base_model_id.replace(\"/\", \"--\")\n",
    "    local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "    print(f\"local model path is: {local_model_path}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_low_bit=\"nf4\",\n",
    "            optimize_model=False,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Model not found locally. Downloading {base_model_id} to cache...\"\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            load_in_low_bit=\"nf4\",\n",
    "            optimize_model=False,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Tokenizer not found locally. Downloading tokenizer for {base_model_id} to cache...\"\n",
    "        )\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777656b-74f4-4ebc-b741-a5d50bc6e79a",
   "metadata": {},
   "source": [
    "## FineTuner\n",
    "\n",
    "The `FineTuner` class encapsulates the entire process of fine-tuning llms for tasks such as text-to-SQL conversion.\n",
    "\n",
    "\n",
    "### Tokenization Strategy\n",
    "\n",
    "The tokenization process is tailored to the type of model being fine-tuned. For instance, if we are working with a Llama model, we utilize a `LlamaTokenizer` to ensure compatibility with the model's expected input format. For other models, a generic `AutoTokenizer` is used. We configure the tokenizer to pad from the left side (`padding_side=\"left\"`) and set the pad token ID to 0.\n",
    "\n",
    "### Data Tokenization and Preparation\n",
    "\n",
    "The `tokenize_data` method is where the fine-tuner ingests raw text data and converts it into a format suitable for training the model. This method handles the addition of end-of-sequence tokens, truncation to a specified `cutoff_len`, and conditioning on the input for training.\n",
    "\n",
    "### Dataset Handling\n",
    "\n",
    "`prepare_data` manages the splitting of data into training and validation sets, applying the `tokenize_data` transformation to each entry. This ensures that our datasets are ready for input into the model, with all necessary tokenization applied.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "Finally, the `train_model` method orchestrates the training process, setting up the `Trainer` with the correct datasets, training arguments, and data collator. The fine-tuning process is encapsulated within the `finetune` method, which strings together all the previous steps into a coherent pipeline, from model setup to training execution.\n",
    "\n",
    "The key steps to using QLoRA for efficient finetuning are:\n",
    "\n",
    "- Load a pretrained model like LLaMA and initialize it in low precision mode by setting load_in_low_bit=\"nf4\". This will load the model with weights quantized to 4-bit NormalFloat (nf4).\n",
    "- Prepare the quantized model for finetuning with prepare_model(model) call. This handles quantizing the model weights into blocks and computing quantization constants.\n",
    "- Add LoRA adapters to the model using get_peft_model(model, config). The config defines the LoRA hyperparameters like adapter size, dropout, etc.\n",
    "- Finetune the model by passing gradients only through the adapters. The base model weights remain fixed in low precision.\n",
    "\n",
    "Looking at the code below:\n",
    "\n",
    "- We using BigDL's AutoModelForCausalLM to load the model with load_in_low_bit=\"nf4\" to initialize in 4-bit.\n",
    "- The model is prepared via prepare_model() which quantizes weights.\n",
    "- LoRA adapters are added with get_peft_model() using the provided config.\n",
    "- Trainer finetunes the model, with gradients only flowing into the adapters.\n",
    "\n",
    "So in summary, we leverage QLoRA in BigDL to load the base LLM in low precision, inject adapters with `peft`, and efficiently finetune by optimizing just the adapters end-to-end while keeping the base model fixed. This unlocks huge memory savings, allowing us to adapt giant models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f4cb8-0da5-4572-bd07-bdae1db897a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    \"\"\"A class to handle the fine-tuning of LLM models.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model_id: str, model_path: str, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialize the FineTuner with base model, model path, and device.\n",
    "\n",
    "        Parameters:\n",
    "            base_model_id (str): Id of pre-trained model to use for fine-tuning.\n",
    "            model_path (str): Path to save the fine-tuned model.\n",
    "            device (torch.device): Device to run the model on.\n",
    "        \"\"\"\n",
    "        self.base_model_id = base_model_id\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id)\n",
    "\n",
    "\n",
    "    def tokenize_data(\n",
    "        self, data_points, add_eos_token=True, train_on_inputs=False, cutoff_len=512\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Tokenizes dataset of SQL related data points consisting of questions, context, and answers.\n",
    "\n",
    "        Parameters:\n",
    "            data_points (dict): A batch from the dataset containing 'question', 'context', and 'answer'.\n",
    "            add_eos_token (bool): Whether to add an EOS token at the end of each tokenized sequence.\n",
    "            cutoff_len (int): The maximum length for each tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question = data_points[\"question\"]\n",
    "            context = data_points[\"context\"]\n",
    "            answer = data_points[\"answer\"]\n",
    "            if train_on_inputs:\n",
    "                user_prompt = generate_prompt_sql(question, context)\n",
    "                tokenized_user_prompt = self.tokenizer(\n",
    "                    user_prompt,\n",
    "                    truncation=True,\n",
    "                    max_length=cutoff_len,\n",
    "                    padding=False,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "                if add_eos_token:\n",
    "                    user_prompt_len -= 1\n",
    "\n",
    "            combined_text = generate_prompt_sql(question, context, answer)\n",
    "            tokenized = self.tokenizer(\n",
    "                combined_text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            if (\n",
    "                tokenized[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "                and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len\n",
    "            ):\n",
    "                tokenized[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            if train_on_inputs:\n",
    "                tokenized[\"labels\"] = [-100] * user_prompt_len + tokenized[\"labels\"][\n",
    "                    user_prompt_len:\n",
    "                ]\n",
    "\n",
    "            return tokenized\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in batch tokenization: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def prepare_data(self, data, val_set_size=100) -> Dataset:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        try:\n",
    "            train_val_split = data[\"train\"].train_test_split(\n",
    "                test_size=val_set_size, shuffle=True, seed=42\n",
    "            )\n",
    "            train_data = train_val_split[\"train\"].shuffle().map(self.tokenize_data)\n",
    "            val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "            return train_data, val_data\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in preparing data: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def train_model(self, train_data, val_data, training_args):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with the given training and validation data.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (Dataset): Training data.\n",
    "            val_data (Optional[Dataset]): Validation data.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model = prepare_model(self.model)\n",
    "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                args=training_args,\n",
    "                data_collator=DataCollatorForSeq2Seq(\n",
    "                    self.tokenizer,\n",
    "                    pad_to_multiple_of=8,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ),\n",
    "            )\n",
    "            self.model.config.use_cache = False\n",
    "            trainer.train()\n",
    "            self.model.save_pretrained(self.model_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model training: {e}\")\n",
    "\n",
    "    def finetune(self, data_path, training_args):\n",
    "        \"\"\"\n",
    "        Execute the fine-tuning pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): Path to the data for fine-tuning.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = load_dataset(data_path)\n",
    "            train_data, val_data = self.prepare_data(data)\n",
    "            self.train_model(train_data, val_data, training_args)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupt received, saving model...\")\n",
    "            self.model.save_pretrained(f\"{self.model_path}_interrupted\")\n",
    "            print(f\"Model saved to {self.model_path}_interrupted\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fintuning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af4187-d362-49b8-bdbc-e8d2a7ae0ac0",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "\n",
    "The `lets_finetune` function below serves as the orchestrator for the fine-tuning process, providing a high-level interface to initiate training with a set of defined parameters. It allows us to specify the device for training, the model to fine-tune, batch size, warm-up steps, learning rate, and the maximum number of steps to take during training. By configuring these parameters, we can tailor the fine-tuning process to fit the specific needs of their dataset and computational resources.\n",
    "\n",
    "Upon invocation, the function sets up the fine-tuning environment, initializes the `FineTuner` class with the selected model and device, and defines the training arguments that control the behavior of the training loop. Once the setup is complete, the `finetuner.finetune` method is called to begin the actual fine-tuning task using the provided dataset. If any errors occur during this process, they are logged for troubleshooting.\n",
    "\n",
    "### Customizing Training Parameters\n",
    "\n",
    "As you proceed with the fine-tuning process, you have the ability to customize a variety of training parameters to suit the needs of your project and the constraints of your computational environment. Below is an overview of key parameters and some considerations for their adjustment:\n",
    "\n",
    "- `per_device_batch_size`: Controls the number of examples processed simultaneously on each device. Optimize this setting to balance between computational efficiency and memory usage.\n",
    "- `gradient_accumulation_steps`: Allows for larger effective batch sizes by accumulating gradients over multiple steps, which can be especially useful when memory is limited.\n",
    "- `warmup_steps`: The number of steps to increase the learning rate at the start of training, which can help in stabilizing the training dynamics.\n",
    "- `save_steps`: How often to save a checkpoint. Frequent checkpoints can be helpful for long training runs where you may want to revert to a previous state.\n",
    "- `max_steps`: Sets the ceiling on the number of training iterations.\n",
    "- `learning_rate`: Influences how quickly the model learns. Finding the right rate is a balance between convergence speed and training stability.\n",
    "- `max_grad_norm`: Used to clip gradients to a maximum norm to prevent the gradients from becoming too large.\n",
    "\n",
    "It's advisable to begin with a high number for `max_steps` (like 1000 or 2000), here i have used `200`, monitoring the training and validation loss to identify the optimal stopping point. If the validation loss begins to increase, indicating overfitting, you can terminate the training early to save computational resources.\n",
    "\n",
    "Interrupting the training process is straightforward in Jupyter—simply select `Kernel -> Interrupt Kernel` if you observe that the model has achieved satisfactory performance before the predefined `max_steps` and then choose the last saved adapter checkpoint from `./lora_adapters` for inference. The latest checkpoint would also be saved to `./final_model_interrupted` directory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a39122-eafc-483c-ad81-9c55de15e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_WANDB = False\n",
    "\n",
    "def lets_finetune(\n",
    "    device=DEVICE,\n",
    "    model=BASE_MODEL,\n",
    "    per_device_batch_size=4,\n",
    "    warmup_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    max_steps=200,\n",
    "    gradient_accum_steps=4,\n",
    "):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        save_steps = 20\n",
    "        eval_steps = 20\n",
    "        max_grad_norm = 0.3\n",
    "        save_total_limit = 3\n",
    "        logging_steps = 20\n",
    "\n",
    "        print(\"\\n\" + \"\\033[1;34m\" + \"=\" * 60 + \"\\033[0m\")\n",
    "        print(\"\\033[1;34mTraining Parameters:\\033[0m\")\n",
    "        param_format = \"\\033[1;34m{:<25} {}\\033[0m\"\n",
    "        print(param_format.format(\"Foundation model:\", BASE_MODEL))\n",
    "        print(param_format.format(\"Model save path:\", MODEL_PATH))\n",
    "        print(param_format.format(\"Device used:\", DEVICE))\n",
    "        if DEVICE.type.startswith(\"xpu\"):\n",
    "            print(param_format.format(\"Intel GPU:\", torch.xpu.get_device_name()))\n",
    "        print(param_format.format(\"Batch size per device:\", per_device_batch_size))\n",
    "        print(param_format.format(\"Gradient accum. steps:\", gradient_accum_steps))\n",
    "        print(param_format.format(\"Warmup steps:\", warmup_steps))\n",
    "        print(param_format.format(\"Save steps:\", save_steps))\n",
    "        print(param_format.format(\"Evaluation steps:\", eval_steps))\n",
    "        print(param_format.format(\"Max steps:\", max_steps))\n",
    "        print(param_format.format(\"Learning rate:\", learning_rate))\n",
    "        print(param_format.format(\"Max gradient norm:\", max_grad_norm))\n",
    "        print(param_format.format(\"Save total limit:\", save_total_limit))\n",
    "        print(param_format.format(\"Logging steps:\", logging_steps))\n",
    "        print(\"\\033[1;34m\" + \"=\" * 60 + \"\\033[0m\\n\")\n",
    "\n",
    "        # Initialize the finetuner with the model and device information\n",
    "        finetuner = FineTuner(\n",
    "            base_model_id=model, model_path=MODEL_PATH, device=device\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accum_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            save_steps=save_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            #max_grad_norm=max_grad_norm,\n",
    "            bf16=True,\n",
    "            #lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length=True,\n",
    "            save_total_limit=save_total_limit,\n",
    "            logging_steps=logging_steps,\n",
    "            optim=\"adamw_hf\",\n",
    "            output_dir=\"./lora_adapters\",\n",
    "            logging_dir=\"./logs\",\n",
    "            report_to=\"wandb\" if ENABLE_WANDB else [],\n",
    "        )\n",
    "\n",
    "        # Start fine-tuning\n",
    "        finetuner.finetune(DATA_PATH, training_args)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc5dee-498b-498a-9ad8-d03f569e9e9a",
   "metadata": {},
   "source": [
    "We can optionally use Weights & Biases to track our training metrics. You will need to pass in your API key when prompted. You can ofcourse skip this step if you'd like to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a31697-8bf5-4ec9-9e13-bb7095218fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = f\"text-to-sql-finetune-model-name_{BASE_MODEL.replace('/', '_')}\"\n",
    "wandb.login()\n",
    "ENABLE_WANDB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497cbaf-994b-474b-92f1-fb33fca6b81f",
   "metadata": {},
   "source": [
    "\n",
    "### Let's Finetune!\n",
    "\n",
    "Now it's time to actually fine-tune the model. The `lets_finetune` function below takes care of this. It initializes a FineTuner object with the configurations you've set or left as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa45c6d-fac1-4331-8dd5-3ebf9cceed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39613af7-3408-4412-8ded-aa6116759f06",
   "metadata": {},
   "source": [
    "### Testing our Fine-Tuned LLM\n",
    "\n",
    "Congratulations on successfully fine-tuning your Language Model for Text-to-SQL tasks! It's now time to put the model to the test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb638e-1464-4596-8809-186834b3f277",
   "metadata": {},
   "source": [
    "### TextToSQLGenerator: Generating SQL Queries from Text Prompts\n",
    "\n",
    "**Note**: If you restart the jupyter kernel, you may have to rerun the Initialization cell to import the necessary packages and also the configuration cell that defines BASE_MODELS.\n",
    "\n",
    "The `TextToSQLGenerator` encapsulates the logic for generating SQL queries from natural language prompts. Here's what you need to know about its implementation:\n",
    "\n",
    "#### Initialization and Configuration:\n",
    "\n",
    "Upon initialization, the `TextToSQLGenerator` class allows the selection of the underlying model. By setting the `use_adapter` parameter to `True`, you can enable the fine-tuned LoRA model for inference. Otherwise, the base pre-trained model is used by default.\n",
    "\n",
    "The constructor of the class automatically selects the correct tokenizer based on the model ID, with a special case for the 'llama' models. The model is then loaded with options optimized for performance on CPUs, such as `low_cpu_mem_usage` and `load_in_4bit`, which are beneficial for environments with memory constraints.\n",
    "\n",
    "For LoRA models, the constructor also takes care of loading the LoRA-specific checkpoints, ensuring that the fine-tuned parameters are utilized during inference.\n",
    "\n",
    "#### Generating SQL Queries:\n",
    "\n",
    "The `generate` method is where the actual translation occurs. Given a text prompt, the method encodes the prompt using the tokenizer, ensuring that it fits within the model's maximum length constraints. It then performs inference to generate the SQL query.\n",
    "\n",
    "The method parameters like `temperature` and `repetition_penalty` which we can tweak to control the creativity and quality of the generated queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a0e504-46b7-4c6e-8d80-b28798607ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "INFERENCE_DEVICE = torch.device(\"cpu\")  \n",
    "\n",
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(base_model_id: str):\n",
    "    \"\"\"Downloads / Load the pre-trained model in 4bit and tokenizer based on the given base model ID for inference.\"\"\"\n",
    "    local_model_id = base_model_id.replace(\"/\", \"--\")\n",
    "    local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "    print(f\"local model path is: {local_model_path}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_4bit=True,\n",
    "            optimize_model=True,\n",
    "            use_cache=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Model not found locally. Downloading {base_model_id} to cache...\"\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_4bit=True,\n",
    "            optimize_model=True,\n",
    "            use_cache=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Tokenizer not found locally. Downloading tokenizer for {base_model_id} to cache...\"\n",
    "        )\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer\n",
    "\n",
    "class TextToSQLGenerator:\n",
    "    \"\"\"Handles SQL query generation for a given text prompt.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_model_id=BASE_MODEL, use_adapter=False, lora_checkpoint=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the InferenceModel class.\n",
    "        Parameters:\n",
    "            use_adapter (bool, optional): Whether to use LoRA model. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id)\n",
    "            if use_adapter:\n",
    "                self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during model initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.model.to(INFERENCE_DEVICE)\n",
    "        self.max_length = 512\n",
    "\n",
    "\n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generates an SQL query based on the given prompt.\n",
    "        Parameters:\n",
    "            prompt (str): The SQL prompt.\n",
    "        Returns:\n",
    "            str: The generated SQL query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_prompt = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.to(INFERENCE_DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with torch.xpu.amp.autocast():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=encoded_prompt,\n",
    "                        do_sample=True,\n",
    "                        max_length=self.max_length,\n",
    "                        temperature=0.3,\n",
    "                        repetition_penalty=1.2,\n",
    "                    )\n",
    "            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during query generation: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1c247-f6e2-4ccd-bdc6-26f41ea63c55",
   "metadata": {},
   "source": [
    "### Generate SQL from Natural Language!\n",
    "\n",
    "With `TextToSQLGenerator`, we can easily compare the performance of the base model and the LoRA-enhanced model by instantiating two objects of the class with different `use_adapter` settings. This facilitates a side-by-side comparison of the generated SQL queries, demonstrating the effectiveness of the fine-tuning process.\n",
    "\n",
    "#### Steps you can follow:\n",
    "\n",
    "1. **Select or use a Natural Language Question**: Start with a natural language question or prompt that you want to translate into an SQL query along with a schema as shown in the `samples` below. You can either choose from the preloaded sample data or use your own.\n",
    "\n",
    "2. **Generate the SQL Query Using the Base Model**: Use the base model to generate an SQL query based on your natural language prompt. This will serve as a baseline for comparison.\n",
    "\n",
    "3. **Generate the SQL Query Using the Fine-Tuned Model**: Next, generate the SQL query using the fine-tuned model. Observe the differences and improvements in the SQL query generated by the fine-tuned model.\n",
    "\n",
    "4. **Compare the Outputs**: Look at the SQL queries generated by both models. Does the fine-tuned model capture the intent of the natural language prompt more accurately? Is the SQL query syntactically correct and optimized?\n",
    "\n",
    "5. **Iterate and Refine**: Fine-tuning is an iterative process. If the outputs aren't as expected, consider revising the training parameters or dataset and fine-tune again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8417146-0c28-4996-81a4-c4b0857d81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# let's use some fake sample data\n",
    "samples = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"question\": \"What is the capacity of the stadium where the team 'Mountain Eagles' plays?\",\n",
    "    \"context\": \"CREATE TABLE stadium_info (team_name VARCHAR, stadium_name VARCHAR, capacity INT)\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How many goals did player John Smith score last season?\",\n",
    "    \"context\": \"CREATE TABLE player_stats (player_name VARCHAR, goals_scored INT, season VARCHAR)\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are the operating hours for the Central Library on weekends?\",\n",
    "    \"context\": \"CREATE TABLE library_hours (library_name VARCHAR, day_of_week VARCHAR, open_time TIME, close_time TIME)\"\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_inference(sample_data, model):\n",
    "  if INFERENCE_DEVICE.type.startswith(\"xpu\"):\n",
    "      torch.xpu.empty_cache()\n",
    "  for row in sample_data:\n",
    "      try:\n",
    "          prompt = generate_prompt_sql(row[\"question\"], context=row[\"context\"])\n",
    "          output = model.generate(prompt)\n",
    "          print(f\"\\nmodel response: {output}\\n\")\n",
    "      except Exception as e:\n",
    "          logging.error(f\"Exception occurred during sample processing: {e}\")\n",
    "\n",
    "\n",
    "# other checkpoints are saved to `./lora_adapters`, if the kernel is interrupted the latest model adapter is saved to `./final_model_interrupted/`\n",
    "LORA_CHECKPOINT = \"./final_model_interrupted/\"\n",
    "model = TextToSQLGenerator(\n",
    "    use_adapter=True,\n",
    "    lora_checkpoint=LORA_CHECKPOINT,\n",
    ")  # set use_adapter=False to use the base model\n",
    "sample_data = json.loads(samples)\n",
    "run_inference(sample_data, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Well done! You've successfully navigated through the process of selecting a foundational LLM model, fine-tuning it using a parameter-efficient approach, and evaluating its SQL generation capabilities on Intel GPUs. This notebook showcases the practical applications and potential of fine-tuning language models for specific tasks. The next steps involve exploring a range of models, adjusting inference settings, and experimenting with various LoRA configurations to further refine your results. Continue on this path of exploration and adaptation to craft solutions that best fit your unique requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "For detailed information on each model's capabilities, licensing, and attribution, please refer to the respective model cards:\n",
    "\n",
    "1. **Open LLaMA 3B v2**\n",
    "   - Model Card: [openlm-research/open_llama_3b_v2](https://huggingface.co/openlm-research/open_llama_3b_v2)\n",
    "\n",
    "2. **Open LLaMA 13B**\n",
    "   - Model Card: [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b)\n",
    "\n",
    "3. **Open LLaMA 7B v2**\n",
    "   - Model Card: [openlm-research/open_llama_7b_v2](https://huggingface.co/openlm-research/open_llama_7b_v2)\n",
    "\n",
    "4. **Nous-Hermes LLaMA 2-7B**\n",
    "   - Model Card: [NousResearch/Nous-Hermes-llama-2-7b](https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b)\n",
    "\n",
    "5. **LLaMA 2-7B Chat HF**\n",
    "   - Model Card: [NousResearch/Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf)\n",
    "\n",
    "6. **LLaMA 2-13B HF**\n",
    "   - Model Card: [NousResearch/Llama-2-13b-hf](https://huggingface.co/NousResearch/Llama-2-13b-hf)\n",
    "\n",
    "7. **CodeLlama 7B HF**\n",
    "   - Model Card: [NousResearch/CodeLlama-7b-hf](https://huggingface.co/NousResearch/CodeLlama-7b-hf)\n",
    "\n",
    "8. **Phind-CodeLlama 34B v2**\n",
    "   - Model Card: [Phind/Phind-CodeLlama-34B-v2](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)\n",
    "\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    " \n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
