{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc557a5-5a55-4bae-8555-a2b655abfa4a",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad2a68-5e1a-491d-9ad8-e0d8eaf067c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not installed, install the bigDL library and others for Intel XPUs:\n",
    "#pip install --pre --upgrade bigdl-llm[xpu]==2.4.0b20231028 -f https://developer.intel.com/ipex-whl-stable-xpu\n",
    "#pip install peft --no-deps\n",
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c23f99-bfcf-476b-91d5-4f2ec1178707",
   "metadata": {},
   "source": [
    "# Text to SQL Generation: Fine-Tuning LLMs with QLoRA on Intel\n",
    "\n",
    "## Applying Quantized LoRA and Hugging Face Transformers on Intel Architecture\n",
    "\n",
    "In this Jupyter Notebook, we will walkthrough the process of fine-tuning a large language model (LLM) to improve its capabilities in generating SQL queries from natural language input. Our focus is on leveraging Intel's hardware, specifically designed for AI and ML workloads, to make this process as efficient as possible. The notebook is suitable for AI engineers and practitioners looking to tune LLMs for specialized tasks such as Text-to-SQL conversions. \n",
    "\n",
    "### What you will learn with this Notebook\n",
    "- Fine-tune a Language Model with either a pre-existing dataset or a custom dataset tailored to your needs on Intel Hw.\n",
    "- Gain insights into the fine-tuning process, including how to manipulate various training parameters to optimize your model's performance.\n",
    "- Test different configurations and observe the results in real-time.\n",
    "\n",
    "### Hardware Compatibility\n",
    "- The notebook is compatible with 4th Generation Intel® Xeon® Scalable Processors and Intel® Data Center GPU Max Series.\n",
    "  \n",
    "### Fine-Tuning with QLoRA: Balancing Memory Efficiency and Adaptability\n",
    "\n",
    "In this notebook, we leverage the QLoRA methodology for fine-tuning, enabling the loading and refinement of large language models within the constraints of available GPU memory. QLoRA, which stands for Quantized Low Rank Adapter, achieves this by applying a clever combination of weight quantization and adapter-based finetuning.\n",
    "\n",
    "The core idea behind QLoRA is to reduce the memory footprint by quantizing the weights of a pre-trained model—compressing them down to a fraction of their original size. This quantization allows us to operate with models that would otherwise exceed our hardware limits. During the fine-tuning process, we optimize only the adapter parameters, which are low-rank matrices specifically designed to update the model for our target task without the need to retrain the entire network. This selective training approach also contributes to computational efficiency, as it narrows down the number of trainable parameters.\n",
    "\n",
    "Through this technique, we can adapt a model to generate SQL queries from natural language with minimal overhead. The BigDL library, Hugging Face Transformers and the peft library provide the necessary tools for this process, facilitating the fine-tuning of language models in a way that is both resource-conscious and effective.\n",
    "\n",
    "Let's begin our journey into fine-tuning a model that can adeptly translate natural language queries into SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362a562-f3ce-4dce-9678-ce317e554a04",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Import all the necessary libraries required for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb9cf2-abcf-48f8-918b-37a18f85ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from math import ceil\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"intel_extension_for_pytorch\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"torchvision.io.image\", lineno=13\n",
    ")\n",
    "warnings.filterwarnings('ignore', message='You are using the default legacy behaviour')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*Parameter.*')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message='This implementation of AdamW is deprecated')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bigdl\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from bigdl.llm.transformers.qlora import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training as prepare_model,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from bigdl.llm.transformers.qlora import PeftModel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "#transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df8dce-a195-430f-a64a-75923990565b",
   "metadata": {},
   "source": [
    "## Note on Model Storage Management\n",
    "\n",
    "The notebook supports certain models out-of-the-box as sated above. However, if you're interested in experimenting with additional models, consider the following guidelines:\n",
    "\n",
    "- **Storage Limitations:** Every user has a default free storage quota. You'll need to ensure that you have enough available space for the models you want to use, especially if your local directory is already storing data.\n",
    "- **Model Support by PEFT:** If the model you're interested in is supported by the `peft` library by default, the necessary LoRA target modules are predefined and can be found in the [PEFT repository](https://github.com/huggingface/peft/blob/main/src/peft/utils/other.py#L434).\n",
    "- **Using Custom Models:** For models not natively supported by `peft`, you'll need to manually set the LoRA target modules in the `LoraConfig`. As an example, for llama models, the relevant target modules would be: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`.\n",
    "- **Checking and Retrieving Disk Space:** To check the available disk space where Hugging Face models are cached, use the Python function. If you need to free up space, you can delete the cache directory, but be aware that this will remove all downloaded models and they will have to be re-downloaded when needed next time.\n",
    "- Also rest the variable in the **Model Configuration** cell to  `MODEL_CACHE_PATH = \"~/\"`\n",
    "\n",
    "### Python Function to Check Disk Space\n",
    "\n",
    "```python\n",
    "# Function to check available disk space in the Hugging Face cache directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_disk_space(path=\"~/.cache/huggingface/\"):\n",
    "    abs_path = os.path.expanduser(path)\n",
    "    total, used, free = shutil.disk_usage(abs_path)\n",
    "    print(f\"Total: {total // (2**30)} GiB\")\n",
    "    print(f\"Used: {used // (2**30)} GiB\")\n",
    "    print(f\"Free: {free // (2**30)} GiB\")\n",
    "\n",
    "# Example usage\n",
    "check_disk_space()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131864cb-ce5d-405b-8886-5d0f1f487c30",
   "metadata": {},
   "source": [
    "## Tailoring Your Model Configuration\n",
    "\n",
    "\n",
    "Welcome to the heart of customization in model fine-tuning. As you embark on this practical journey, you have a suite of base models at your disposal. Each has unique strengths, and the choice depends on your specific finetuning goals.\n",
    "\n",
    "- **Your Model Options**: Within the `BASE_MODELS` array, you’ll find options ranging from the nimble \n",
    "`open_llama_3b_v2` to the more expansive `open_llama_7b_v2`, and specialized variants like `CodeLlama-7b-hf`. \n",
    "Feel free to switch between these models to discover which one aligns best with your objectives.\n",
    "\n",
    "- **Dataset**: We will be using `b-mc2/sql-create-context` dataset from Huggingface datasets, that encompasses 78,577 examples, including natural language queries, SQL CREATE TABLE statements, and corresponding SQL queries. This dataset, extending from WikiSQL and Spider, is designed with text-to-SQL models in mind. It provides the structure needed for models to understand and generate accurate SQL statements. More details on the dataset can be found [here](https://huggingface.co/datasets/b-mc2/sql-create-context).\n",
    "  \n",
    "- **LoRA Parameters - Your Knobs to Turn**:\n",
    "  - `r` (Rank): This is a key factor in how finely your model can adapt. A higher rank can grasp more \n",
    "  complex nuances, while a lower rank ensures a leaner memory footprint.\n",
    "  - `lora_alpha` (Scaling Factor): Think of this as the volume control for the LoRA adapters’ influence \n",
    "  on your model. Adjusting `lora_alpha` helps you fine-tune the balance between adaptation and preserving \n",
    "  the integrity of the pre-trained weights.\n",
    "  - `target_modules`: You decide which parts of the transformer model to enhance with LoRA adapters, \n",
    "  directly impacting how your model interprets and generates language.\n",
    "  - `lora_dropout`: A parameter that helps your model stay robust against overfitting. You can experiment \n",
    "  with different rates to achieve the best generalization.\n",
    "  - `bias`: Tweak the bias configurations to see their effect on your model’s learning dynamics.\n",
    "\n",
    "This notebook is set to start with `open_llama_7b_v2` as the default model, striking a balance suitable for a wide range of tasks. However, this journey is yours, and the real power lies with you to explore and fine-tune these settings to perfection on your chosen hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88077cc2-8fcf-4128-ac44-9fb2bb327398",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODELS = {\n",
    "    \"0\": \"openlm-research/open_llama_3b_v2\", # https://huggingface.co/openlm-research/open_llama_3b_v2\n",
    "    \"1\": \"openlm-research/open_llama_7b_v2\", # https://huggingface.co/openlm-research/open_llama_7b_v2\n",
    "    \"2\": \"NousResearch/Nous-Hermes-Llama-2-7b\", # https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b\n",
    "    \"3\": \"NousResearch/Llama-2-7b-chat-hf\", # https://huggingface.co/NousResearch/Llama-2-7b-chat-hf\n",
    "    \"4\": \"NousResearch/CodeLlama-7b-hf\", # https://huggingface.co/NousResearch/CodeLlama-7b-hf\n",
    "}\n",
    "BASE_MODEL = BASE_MODELS[\"0\"]\n",
    "DATA_PATH = \"b-mc2/sql-create-context\"\n",
    "MODEL_PATH = \"./final_model\"\n",
    "ADAPTER_PATH = \"./lora_adapters\"\n",
    "DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "MODEL_CACHE_PATH = \"./bigdata/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797b621-fd06-4ae9-a883-d7d15f16d6c4",
   "metadata": {},
   "source": [
    "## Prompt Engineering for Text-to-SQL Conversion\n",
    "\n",
    "In the realm of fine-tuning language models for specialized tasks, the design of the prompt is pivotal. The function `generate_prompt_sql` plays a crucial role in this process, particularly for the task of converting natural language questions into SQL queries.The `generate_prompt_sql` function is crafted to encapsulate the input question, the relevant database context, and the expected output in a structured and concise manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecd7df-b7ce-48b0-ba9f-04b7ec0c1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777656b-74f4-4ebc-b741-a5d50bc6e79a",
   "metadata": {},
   "source": [
    "## FineTuner\n",
    "\n",
    "The `FineTuner` class encapsulates the entire process of fine-tuning llms for tasks such as text-to-SQL conversion.\n",
    "\n",
    "## Model Loading and Configuration\n",
    "\n",
    "When initializing the `FineTuner`, we load the base model specified by the `base_model_id`. Crucially, we use the `load_in_low_bit` parameter with the value \"nf4\", which allows the model to load in a 4-bit format, significantly reducing memory footprint while maintaining performance. \n",
    "\n",
    "The model is also set to use a `torch_dtype` of `torch.float16`, enabling mixed-precision training for faster computation and lower memory usage. .\n",
    "\n",
    "## Tokenization Strategy\n",
    "The tokenization process is tailored to the type of model being fine-tuned. For instance, if we are working with a Llama model, we utilize a `LlamaTokenizer` to ensure compatibility with the model's expected input format. For other models, a generic `AutoTokenizer` is used.\n",
    "\n",
    "We configure the tokenizer to pad from the left side (`padding_side=\"left\"`) and set the pad token ID to 0, which is a common practice for certain language models that are sensitive to the position and order of tokens.\n",
    "\n",
    "## Data Tokenization and Preparation\n",
    "The `tokenize_data` method is where the fine-tuner ingests raw text data and converts it into a format suitable for training the model. This method handles the addition of end-of-sequence tokens, truncation to a specified `cutoff_len`, and conditioning on the input for training.\n",
    "\n",
    "## Dataset Handling\n",
    "`prepare_data` manages the splitting of data into training and validation sets, applying the `tokenize_data` transformation to each entry. This ensures that our datasets are ready for input into the model, with all necessary tokenization applied.\n",
    "\n",
    "## Training Process\n",
    "Finally, the `train_model` method orchestrates the training process, setting up the `Trainer` with the correct datasets, training arguments, and data collator.\n",
    "\n",
    "The fine-tuning process is encapsulated within the `finetune` method, which strings together all the previous steps into a coherent pipeline, from model setup to training execution.\n",
    "\n",
    "\n",
    "The key steps to using QLoRA for efficient finetuning are:\n",
    "\n",
    "- Load a pretrained model like LLaMA and initialize it in low precision mode by setting load_in_low_bit=\"nf4\". This will load the model with weights quantized to 4-bit NormalFloat (nf4).\n",
    "- Prepare the quantized model for finetuning with prepare_model(model) call. This handles quantizing the model weights into blocks and computing quantization constants.\n",
    "- Add LoRA adapters to the model using get_peft_model(model, config). The config defines the LoRA hyperparameters like adapter size, dropout, etc.\n",
    "- Finetune the model by passing gradients only through the adapters. The base model weights remain fixed in low precision.\n",
    "\n",
    "Looking at the code:\n",
    "\n",
    "- We using BigDL's AutoModelForCausalLM to load the model with load_in_low_bit=\"nf4\" to initialize in 4-bit.\n",
    "- The model is prepared via prepare_model() which quantizes weights into blocks.\n",
    "- LoRA adapters are added with get_peft_model() using the provided config.\n",
    "- Trainer finetunes the model, with gradients only flowing into the adapters.\n",
    "\n",
    "So in summary, we leverage QLoRA in BigDL to load the base LLM in low precision, inject adapters, and efficiently finetune by optimizing just the adapters end-to-end while keeping the base model fixed. This unlocks huge memory savings, allowing us to adapt giant models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff116075-b33f-4ea4-9c19-8494d3ef24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models...\n",
    "def save_models_to_directory(base_models, save_directory):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)    \n",
    "    for model_id in base_models.values():\n",
    "        local_model_name = model_id.replace(\"/\", \"--\")\n",
    "        local_model_path = os.path.join(save_directory, local_model_name)        \n",
    "        if not os.path.exists(local_model_path):\n",
    "            print(f\"Downloading and saving {model_id} to {local_model_path}...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            model.save_pretrained(local_model_path)\n",
    "            tokenizer.save_pretrained(local_model_path)\n",
    "        else:\n",
    "            print(f\"Model {model_id} already saved in {local_model_path}\")\n",
    "\n",
    "#MODEL_CACHE_DIR = \"./model_cache\"\n",
    "#save_models_to_directory(BASE_MODELS, MODEL_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f4cb8-0da5-4572-bd07-bdae1db897a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    \"\"\"A class to handle the fine-tuning of LLM models.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model_id: str, model_path: str, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialize the FineTuner with base model, model path, and device.\n",
    "\n",
    "        Parameters:\n",
    "            base_model_id (str): Id of pre-trained model to use for fine-tuning.\n",
    "            model_path (str): Path to save the fine-tuned model.\n",
    "            device (torch.device): Device to run the model on.\n",
    "        \"\"\"\n",
    "        self.base_model_id = base_model_id\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def setup_models(self):\n",
    "        \"\"\"Downloads the pre-trained model and tokenizer based on the given base model ID.\"\"\"\n",
    "        local_model_id = self.base_model_id.replace(\"/\", \"--\")\n",
    "        local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                local_model_path,\n",
    "                load_in_low_bit=\"nf4\",\n",
    "                optimize_model=False,\n",
    "                torch_dtype=torch.float16,\n",
    "                modules_to_not_convert=[\"lm_head\"]\n",
    "            )\n",
    "        except OSError:\n",
    "            logging.info(f\"Model not found locally. Downloading {self.base_model_id} to cache...\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_id,\n",
    "                load_in_low_bit=\"nf4\",\n",
    "                optimize_model=False,\n",
    "                torch_dtype=torch.float16,\n",
    "                modules_to_not_convert=[\"lm_head\"]\n",
    "            )\n",
    "            self.model.save_pretrained(local_model_path)\n",
    "\n",
    "        try:\n",
    "            if 'llama' in self.base_model_id.lower():\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "        except OSError:\n",
    "            logging.info(f\"Tokenizer not found locally. Downloading tokenizer for {self.base_model_id} to cache...\")\n",
    "            if 'llama' in self.base_model_id.lower():\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(self.base_model_id)\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id)\n",
    "            self.tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "    def tokenize_data(\n",
    "        self, data_points, add_eos_token=True, train_on_inputs=False, cutoff_len=512\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Tokenizes dataset of SQL related data points consisting of questions, context, and answers.\n",
    "\n",
    "        Parameters:\n",
    "            data_points (dict): A batch from the dataset containing 'question', 'context', and 'answer'.\n",
    "            add_eos_token (bool): Whether to add an EOS token at the end of each tokenized sequence.\n",
    "            cutoff_len (int): The maximum length for each tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question = data_points[\"question\"]\n",
    "            context = data_points[\"context\"]\n",
    "            answer = data_points[\"answer\"]\n",
    "            if train_on_inputs:\n",
    "                user_prompt = generate_prompt_sql(question, context)\n",
    "                tokenized_user_prompt = self.tokenizer(\n",
    "                    user_prompt,\n",
    "                    truncation=True,\n",
    "                    max_length=cutoff_len,\n",
    "                    padding=False,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "                if add_eos_token:\n",
    "                    user_prompt_len -= 1\n",
    "\n",
    "            combined_text = generate_prompt_sql(question, context, answer)\n",
    "            tokenized = self.tokenizer(\n",
    "                combined_text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            if (\n",
    "                tokenized[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "                and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len\n",
    "            ):\n",
    "                tokenized[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            if train_on_inputs:\n",
    "                tokenized[\"labels\"] = [-100] * user_prompt_len + tokenized[\"labels\"][\n",
    "                    user_prompt_len:\n",
    "                ]\n",
    "\n",
    "            return tokenized\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in batch tokenization: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def prepare_data(self, data, val_set_size=100) -> Dataset:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        try:\n",
    "            train_val_split = data[\"train\"].train_test_split(\n",
    "                test_size=val_set_size, shuffle=True, seed=42\n",
    "            )\n",
    "            train_data = train_val_split[\"train\"].shuffle().map(self.tokenize_data)\n",
    "            val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "            return train_data, val_data\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in preparing data: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def train_model(self, train_data, val_data, training_args):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with the given training and validation data.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (Dataset): Training data.\n",
    "            val_data (Optional[Dataset]): Validation data.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.model = prepare_model(self.model)\n",
    "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                args=training_args,\n",
    "                data_collator=DataCollatorForSeq2Seq(\n",
    "                    self.tokenizer,\n",
    "                    pad_to_multiple_of=8,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ),\n",
    "            )\n",
    "            self.model.config.use_cache = False\n",
    "            trainer.train()\n",
    "            self.model.save_pretrained(self.model_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model training: {e}\")\n",
    "\n",
    "    def finetune(self, data_path, training_args):\n",
    "        \"\"\"\n",
    "        Execute the fine-tuning pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): Path to the data for fine-tuning.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.setup_models()\n",
    "            data = load_dataset(data_path)\n",
    "            train_data, val_data = self.prepare_data(data)\n",
    "            self.train_model(train_data, val_data, training_args)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupt received, saving model...\")\n",
    "            self.model.save_pretrained(f\"{self.model_path}_interrupted\")\n",
    "            print(f\"Model saved to {self.model_path}_interrupted\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fintuning: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af4187-d362-49b8-bdbc-e8d2a7ae0ac0",
   "metadata": {},
   "source": [
    "The `lets_finetune` function serves as the orchestrator for the fine-tuning process, providing a high-level interface to initiate training with a set of defined parameters. It allows us to specify the device for training, the model to fine-tune, batch size, warm-up steps, learning rate, and the maximum number of steps to take during training. By configuring these parameters, we can tailor the fine-tuning process to fit the specific needs of their dataset and computational resources.\n",
    "\n",
    "Upon invocation, the function sets up the fine-tuning environment, initializes the `FineTuner` class with the selected model and device, and defines the training arguments that control the behavior of the training loop. These arguments include strategies for saving checkpoints, evaluation frequency, learning rate scheduling, precision training settings, and more. With bf16 precision and efficient batching, the function is geared towards achieving a balance between training speed and memory usage.\n",
    "\n",
    "Once the setup is complete, the `finetuner.finetune` method is called to begin the actual fine-tuning task using the provided dataset. If any errors occur during this process, they are logged for troubleshooting. \n",
    "\n",
    "### Customizing Training Parameters\n",
    "\n",
    "As you proceed with the fine-tuning process, you have the ability to customize a variety of training parameters to suit the needs of your project and the constraints of your computational environment. Below is an overview of key parameters and some considerations for their adjustment:\n",
    "\n",
    "- `per_device_batch_size`: Controls the number of examples processed simultaneously on each device. Optimize this setting to balance between computational efficiency and memory usage.\n",
    "- `gradient_accumulation_steps`: Allows for larger effective batch sizes by accumulating gradients over multiple steps, which can be especially useful when memory is limited.\n",
    "- `warmup_steps`: The number of steps to increase the learning rate at the start of training, which can help in stabilizing the training dynamics.\n",
    "- `save_steps`: How often to save a checkpoint. Frequent checkpoints can be helpful for long training runs where you may want to revert to a previous state.\n",
    "- `max_steps`: Sets the ceiling on the number of training iterations. \n",
    "- `learning_rate`: Influences how quickly the model learns. Finding the right rate is a balance between convergence speed and training stability.\n",
    "- `max_grad_norm`: Used to clip gradients to a maximum norm to prevent the gradients from becoming too large.\n",
    "\n",
    "It's advisable to begin with a high number for `max_steps` (like 1000 or 2000), here i have used `500`, monitoring the training and validation loss to identify the optimal stopping point. If the validation loss begins to increase, indicating overfitting, you can terminate the training early to save computational resources.\n",
    "\n",
    "Interrupting the training process is straightforward in Jupyter—simply select `Kernel -> Interrupt Kernel` if you observe that the model has achieved satisfactory performance before the predefined `max_steps` and then choose the last saved adapter checkpoint from `./lora_adapters` for inference. The latest checkpoint would also be saved to `./final_model_interrupted` directory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a39122-eafc-483c-ad81-9c55de15e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_WANDB = False\n",
    "def lets_finetune(device=DEVICE, model=BASE_MODEL, per_device_train_batch_size=2, warmup_steps=100, learning_rate=3e-4, max_steps=200):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        per_device_batch_size = 2\n",
    "        gradient_accum_steps = 4\n",
    "        warmup_steps = 100\n",
    "        save_steps = 20\n",
    "        eval_steps = 20\n",
    "        max_steps = 500\n",
    "        learning_rate = 3e-4\n",
    "        max_grad_norm = 0.3\n",
    "        save_total_limit = 3\n",
    "        logging_steps = 20\n",
    "\n",
    "        print(\"\\n\" + \"\\033[1;34m\" + \"=\" * 60 + \"\\033[0m\")\n",
    "        print(\"\\033[1;34mTraining Parameters:\\033[0m\") \n",
    "        param_format = \"\\033[1;34m{:<25} {}\\033[0m\" \n",
    "        print(param_format.format(\"Foundation model:\", BASE_MODEL))\n",
    "        print(param_format.format(\"Model save path:\", MODEL_PATH))\n",
    "        print(param_format.format(\"Device used:\", DEVICE))\n",
    "        if DEVICE.type.startswith(\"xpu\"):\n",
    "            print(param_format.format(\"Intel GPU:\", torch.xpu.get_device_name()))\n",
    "        print(param_format.format(\"Batch size per device:\", per_device_batch_size))\n",
    "        print(param_format.format(\"Gradient accum. steps:\", gradient_accum_steps))\n",
    "        print(param_format.format(\"Warmup steps:\", warmup_steps))\n",
    "        print(param_format.format(\"Save steps:\", save_steps))\n",
    "        print(param_format.format(\"Evaluation steps:\", eval_steps))\n",
    "        print(param_format.format(\"Max steps:\", max_steps))\n",
    "        print(param_format.format(\"Learning rate:\", learning_rate))\n",
    "        print(param_format.format(\"Max gradient norm:\", max_grad_norm))\n",
    "        print(param_format.format(\"Save total limit:\", save_total_limit))\n",
    "        print(param_format.format(\"Logging steps:\", logging_steps))\n",
    "        print(\"\\033[1;34m\" + \"=\" * 60 + \"\\033[0m\\n\")\n",
    "\n",
    "        # Initialize the finetuner with the model and device information\n",
    "        finetuner = FineTuner(\n",
    "            base_model_id=BASE_MODEL, model_path=MODEL_PATH, device=DEVICE\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accum_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            save_steps=save_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            bf16=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length=True,\n",
    "            save_total_limit=save_total_limit,\n",
    "            logging_steps=logging_steps,\n",
    "            optim=\"adamw_hf\",\n",
    "            output_dir=\"./lora_adapters\",\n",
    "            logging_dir=\"./logs\",\n",
    "            report_to=\"wandb\" if ENABLE_WANDB else [],\n",
    "        )\n",
    "\n",
    "        # Start fine-tuning\n",
    "        finetuner.finetune(DATA_PATH, training_args)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc5dee-498b-498a-9ad8-d03f569e9e9a",
   "metadata": {},
   "source": [
    "We can optionally use Weights & Biases to track our training metrics. You will need to pass in your API key when prompted. You can ofcourse skip this step if you'd like to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a31697-8bf5-4ec9-9e13-bb7095218fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "ENABLE_WANDB = True\n",
    "os.environ[\"WANDB_PROJECT\"] = \"text-to-sql-finetune\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497cbaf-994b-474b-92f1-fb33fca6b81f",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "Now it's time to actually fine-tune the model. The `lets_finetune` function below takes care of this. It initializes a FineTuner object with the configurations you've set or left as default.\n",
    "\n",
    "### What Does It Do?\n",
    "- Initializes the FineTuner object with the base model and other configurations.\n",
    "- Sets up training arguments like batch size, learning rate, evaluation steps, etc.\n",
    "- Starts the fine-tuning process using the data and configurations provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa45c6d-fac1-4331-8dd5-3ebf9cceed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39613af7-3408-4412-8ded-aa6116759f06",
   "metadata": {},
   "source": [
    "### Testing our Fine-Tuned LLM\n",
    "\n",
    "Congratulations on successfully fine-tuning your Language Model for Text-to-SQL tasks! It's now time to put the model to the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb638e-1464-4596-8809-186834b3f277",
   "metadata": {},
   "source": [
    "### TextToSQLGenerator: Generating SQL Queries from Text Prompts\n",
    "\n",
    "\n",
    "**Note**: If you stopped the jupyter kernel, you will have to rerun the Initialization cell to import the necessary packages and also the configuration cell that defines BASE_MODELS. \n",
    "\n",
    "The `TextToSQLGenerator` encapsulates the logic for generating SQL queries from natural language prompts. Here's what you need to know about its implementation:\n",
    "\n",
    "#### Initialization and Configuration:\n",
    "\n",
    "Upon initialization, the `TextToSQLGenerator` class allows the selection of the underlying model. By setting the `use_adapter` parameter to `True`, you can enable the fine-tuned LoRA model for inference. Otherwise, the base pre-trained model is used by default.\n",
    "\n",
    "The constructor of the class automatically selects the correct tokenizer based on the model ID, with a special case for the 'llama' models. The model is then loaded with options optimized for performance on CPUs, such as `low_cpu_mem_usage` and `load_in_4bit`, which are beneficial for environments with memory constraints.\n",
    "\n",
    "For LoRA models, the constructor also takes care of loading the LoRA-specific checkpoints, ensuring that the fine-tuned parameters are utilized during inference.\n",
    "\n",
    "#### Generating SQL Queries:\n",
    "\n",
    "The `generate` method is where the actual translation occurs. Given a text prompt, the method encodes the prompt using the tokenizer, ensuring that it fits within the model's maximum length constraints. It then performs inference to generate the SQL query.\n",
    "\n",
    "The method parameters like `temperature` and `repetition_penalty` which we can tweak to control the creativity and quality of the generated queries!\n",
    "\n",
    "#### Usage:\n",
    "\n",
    "With `TextToSQLGenerator`, we can easily compare the performance of the base model and the LoRA-enhanced model by instantiating two objects of the class with different `use_adapter` settings. This facilitates a side-by-side comparison of the generated SQL queries, demonstrating the effectiveness of the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0e504-46b7-4c6e-8d80-b28798607ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "\n",
    "class TextToSQLGenerator:\n",
    "    \"\"\"Handles SQL query generation for a given text prompt.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model_id=BASE_MODEL, use_adapter=False):\n",
    "        \"\"\"\n",
    "        Initialize the InferenceModel class.\n",
    "        Parameters:\n",
    "            use_adapter (bool, optional): Whether to use LoRA model. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.base_model_id = base_model_id\n",
    "            # Choose the appropriate tokenizer based on the model name\n",
    "            if 'llama' in self.base_model_id.lower():\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(self.base_model_id)\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_id)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_id,\n",
    "                low_cpu_mem_usage=False,\n",
    "                load_in_4bit=True,\n",
    "                optimize_model=False,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            if use_adapter:\n",
    "                self.model = PeftModel.from_pretrained(self.model, LORA_CHECKPOINT)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during model initialization: {e}\")\n",
    "            raise\n",
    "            \n",
    "        self.model.to(DEVICE)\n",
    "        self.max_length = 512\n",
    "        self.tokenizer.pad_token_id = 0\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generates an SQL query based on the given prompt.\n",
    "        Parameters:\n",
    "            prompt (str): The SQL prompt.\n",
    "        Returns:\n",
    "            str: The generated SQL query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_prompt = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with torch.xpu.amp.autocast():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=encoded_prompt,\n",
    "                        do_sample=True,\n",
    "                        max_length=self.max_length,\n",
    "                        temperature=0.3,\n",
    "                        repetition_penalty=1.2,\n",
    "                    )\n",
    "            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during query generation: {e}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1c247-f6e2-4ccd-bdc6-26f41ea63c55",
   "metadata": {},
   "source": [
    "\n",
    "#### Steps to Follow:\n",
    "1. **Select or use a Natural Language Query**: Start with a natural language question or prompt that you want to translate into an SQL query. You can either choose from the preloaded sample data or use your own.\n",
    "\n",
    "2. **Generate the SQL Query Using the Base Model**: Use the base model to generate an SQL query based on your natural language prompt. This will serve as a baseline for comparison.\n",
    "\n",
    "3. **Generate the SQL Query Using the Fine-Tuned Model**: Next, generate the SQL query using the fine-tuned model. Observe the differences and improvements in the SQL query generated by the fine-tuned model.\n",
    "\n",
    "4. **Compare the Outputs**: Look at the SQL queries generated by both models. Does the fine-tuned model capture the intent of the natural language prompt more accurately? Is the SQL query syntactically correct and optimized?\n",
    "\n",
    "5. **Iterate and Refine**: Fine-tuning is an iterative process. If the outputs aren't as expected, consider revising the training parameters or dataset and fine-tune again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8417146-0c28-4996-81a4-c4b0857d81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# let's use some fake sample data\n",
    "samples = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"question\": \"What is the capacity of the stadium where the team 'Mountain Eagles' plays?\",\n",
    "    \"context\": \"CREATE TABLE stadium_info (team_name VARCHAR, stadium_name VARCHAR, capacity INT)\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How many goals did player John Smith score last season?\",\n",
    "    \"context\": \"CREATE TABLE player_stats (player_name VARCHAR, goals_scored INT, season VARCHAR)\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are the operating hours for the Central Library on weekends?\",\n",
    "    \"context\": \"CREATE TABLE library_hours (library_name VARCHAR, day_of_week VARCHAR, open_time TIME, close_time TIME)\"\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def run_inference(sample_data, model):\n",
    "    torch.xpu.empty_cache()\n",
    "    for row in sample_data:\n",
    "        try:\n",
    "            prompt = generate_prompt_sql(row[\"question\"], context=row[\"context\"])\n",
    "            output = model.generate(prompt)\n",
    "            print(f\"\\nmodel response: {output}\\n\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during sample processing: {e}\")\n",
    "\n",
    "# other checkpoints are saved to `./lora_adapters`, if the kernel is interrupted the latest model adapter is saved to `./final_model_interrupted/`\n",
    "LORA_CHECKPOINT = \"./final_model_interrupted/\"\n",
    "model = TextToSQLGenerator(use_adapter=True)  # se use_adapter=False to get the base model's output\n",
    "sample_data = json.loads(samples)\n",
    "run_inference(sample_data, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d2048-0bf5-407d-9fee-f24fb38b60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# license and usage recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ef436-2abe-41d2-a6c1-63af3035d654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
